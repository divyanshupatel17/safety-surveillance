{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Fire and Smoke Detection YOLOv8 Training Pipeline\n",
        "\n",
        "Train YOLOv8 model to detect fire and smoke for safety surveillance.\n",
        "\n",
        "Model saves to:\n",
        "- Kaggle: /kaggle/working/models/fire_smoke/best.pt\n",
        "- Colab: /content/drive/MyDrive/safety-models/fire_smoke/best.pt\n",
        "- Local: ./models/fire_smoke/best.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step1_title",
      "metadata": {},
      "source": [
        "## Step 1: Environment Setup and GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    IS_COLAB = 'google.colab' in str(get_ipython())\n",
        "except:\n",
        "    IS_COLAB = False\n",
        "\n",
        "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
        "\n",
        "if IS_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_DIR = '/content/drive/MyDrive/safety-models'\n",
        "    DATASET_PATH = '/content/fire-smoke-dataset'\n",
        "    WORKING_DIR = '/content'\n",
        "    print('Running on Colab')\n",
        "    print('IMPORTANT: Go to Runtime > Change runtime type > Select GPU (T4 or better)')\n",
        "elif IS_KAGGLE:\n",
        "    BASE_DIR = '/kaggle/working/models'\n",
        "    DATASET_PATH = '/kaggle/input/fire-and-smoke-dataset'\n",
        "    WORKING_DIR = '/kaggle/working'\n",
        "    print('Running on Kaggle')\n",
        "    print('IMPORTANT: Go to Settings > Accelerator > Select GPU T4 x2')\n",
        "else:\n",
        "    BASE_DIR = './models'\n",
        "    DATASET_PATH = './datasets/fire_smoke'\n",
        "    WORKING_DIR = '.'\n",
        "    print('Running locally')\n",
        "\n",
        "MODELS_DIR = os.path.join(BASE_DIR, 'fire_smoke')\n",
        "RESULTS_DIR = os.path.join(WORKING_DIR, 'results/fire_smoke')\n",
        "\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nModels: {MODELS_DIR}\")\n",
        "print(f\"Dataset: {DATASET_PATH}\")\n",
        "print(f\"Results: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step2_title",
      "metadata": {},
      "source": [
        "## Step 2: Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "packages = ['ultralytics', 'opencv-python', 'pillow', 'pyyaml']\n",
        "for pkg in packages:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
        "\n",
        "print('Installed')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step3_title",
      "metadata": {},
      "source": [
        "## Step 3: Import Libraries and Verify GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "import json\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "gpu_available = torch.cuda.is_available()\n",
        "device = 0 if gpu_available else 'cpu'\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"GPU Available: {gpu_available}\")\n",
        "\n",
        "if gpu_available:\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"Using Device: GPU (cuda:0)\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No GPU detected!\")\n",
        "    print(\"Training will be VERY SLOW on CPU\")\n",
        "    print(f\"\\nCurrent Device: CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step4_title",
      "metadata": {},
      "source": [
        "## Step 4: Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step4",
      "metadata": {},
      "outputs": [],
      "source": [
        "if IS_COLAB:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'kagglehub'])\n",
        "    import kagglehub\n",
        "    \n",
        "    print('Downloading Fire and Smoke dataset from Kaggle...')\n",
        "    download_path = kagglehub.dataset_download('dataclusterlabs/fire-and-smoke-dataset')\n",
        "    print(f\"Downloaded to: {download_path}\")\n",
        "    \n",
        "    DATASET_PATH = download_path\n",
        "elif IS_KAGGLE:\n",
        "    print('Using Kaggle input dataset')\n",
        "else:\n",
        "    print('Local mode: ensure dataset is in ./datasets/fire_smoke')\n",
        "\n",
        "print(f\"Dataset path: {DATASET_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step5_title",
      "metadata": {},
      "source": [
        "## Step 5: Prepare Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_exists = os.path.exists(DATASET_PATH)\n",
        "print(f\"Dataset exists: {dataset_exists}\")\n",
        "\n",
        "if dataset_exists:\n",
        "    print('\\nAnalyzing dataset structure...')\n",
        "    \n",
        "    all_images = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG']:\n",
        "        all_images.extend(glob.glob(os.path.join(DATASET_PATH, '**', ext), recursive=True))\n",
        "    \n",
        "    print(f\"Found {len(all_images)} images\")\n",
        "    \n",
        "    if len(all_images) > 0:\n",
        "        prepared_dir = os.path.join(WORKING_DIR, 'fire_smoke_prepared')\n",
        "        os.makedirs(prepared_dir, exist_ok=True)\n",
        "        \n",
        "        for split in ['train', 'val', 'test']:\n",
        "            os.makedirs(os.path.join(prepared_dir, 'images', split), exist_ok=True)\n",
        "            os.makedirs(os.path.join(prepared_dir, 'labels', split), exist_ok=True)\n",
        "        \n",
        "        train_imgs, test_imgs = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "        train_imgs, val_imgs = train_test_split(train_imgs, test_size=0.15, random_state=42)\n",
        "        \n",
        "        print(f\"Split: Train={len(train_imgs)}, Val={len(val_imgs)}, Test={len(test_imgs)}\")\n",
        "        \n",
        "        CLASS_NAMES = {0: 'fire', 1: 'smoke'}\n",
        "        \n",
        "        def copy_and_label(img_list, split):\n",
        "            for idx, img_path in enumerate(img_list):\n",
        "                img_name = os.path.basename(img_path)\n",
        "                new_name = f\"{split}_{idx:05d}{os.path.splitext(img_name)[1]}\"\n",
        "                \n",
        "                shutil.copy2(img_path, os.path.join(prepared_dir, 'images', split, new_name))\n",
        "                \n",
        "                label_class = 0 if 'fire' in img_path.lower() else 1\n",
        "                \n",
        "                img = Image.open(img_path)\n",
        "                w, h = img.size\n",
        "                x_center, y_center = 0.5, 0.5\n",
        "                width, height = 0.8, 0.8\n",
        "                \n",
        "                label_path = os.path.join(prepared_dir, 'labels', split, \n",
        "                                         os.path.splitext(new_name)[0] + '.txt')\n",
        "                with open(label_path, 'w') as f:\n",
        "                    f.write(f\"{label_class} {x_center} {y_center} {width} {height}\\n\")\n",
        "        \n",
        "        copy_and_label(train_imgs, 'train')\n",
        "        copy_and_label(val_imgs, 'val')\n",
        "        copy_and_label(test_imgs, 'test')\n",
        "        \n",
        "        DATASET_PATH = prepared_dir\n",
        "        print(f\"\\nDataset prepared at: {DATASET_PATH}\")\n",
        "    else:\n",
        "        print('No images found in dataset')\n",
        "        CLASS_NAMES = {0: 'fire', 1: 'smoke'}\n",
        "else:\n",
        "    print('Dataset not found')\n",
        "    CLASS_NAMES = {0: 'fire', 1: 'smoke'}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step6_title",
      "metadata": {},
      "source": [
        "## Step 6: Create Dataset Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step6",
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(DATASET_PATH):\n",
        "    yaml_path = os.path.join(DATASET_PATH, 'data.yaml')\n",
        "    \n",
        "    data_config = {\n",
        "        'path': DATASET_PATH,\n",
        "        'train': 'images/train',\n",
        "        'val': 'images/val',\n",
        "        'test': 'images/test',\n",
        "        'nc': len(CLASS_NAMES),\n",
        "        'names': list(CLASS_NAMES.values())\n",
        "    }\n",
        "    \n",
        "    with open(yaml_path, 'w') as f:\n",
        "        yaml.dump(data_config, f)\n",
        "    \n",
        "    print('Created data.yaml')\n",
        "    print(f\"Classes: {list(CLASS_NAMES.values())}\")\n",
        "    \n",
        "    for split in ['train', 'val', 'test']:\n",
        "        img_dir = os.path.join(DATASET_PATH, 'images', split)\n",
        "        if os.path.exists(img_dir):\n",
        "            count = len([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png'))])\n",
        "            print(f\"{split}: {count} images\")\n",
        "else:\n",
        "    print('Dataset not prepared')\n",
        "    yaml_path = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step7_title",
      "metadata": {},
      "source": [
        "## Step 7: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step7",
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_SIZE = 's'\n",
        "\n",
        "model = YOLO(f'yolov8{MODEL_SIZE}.pt')\n",
        "params = sum(p.numel() for p in model.model.parameters()) / 1e6\n",
        "\n",
        "print(f\"Model: YOLOv8{MODEL_SIZE}\")\n",
        "print(f\"Parameters: {params:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step8_title",
      "metadata": {},
      "source": [
        "## Step 8: Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step8",
      "metadata": {},
      "outputs": [],
      "source": [
        "if yaml_path is None:\n",
        "    print('Cannot train without dataset')\n",
        "elif not gpu_available:\n",
        "    print('WARNING: No GPU - training will be slow')\n",
        "    print('Reduce epochs and batch size')\n",
        "else:\n",
        "    print('Starting GPU training...')\n",
        "    \n",
        "    results = model.train(\n",
        "        data=yaml_path,\n",
        "        epochs=50,\n",
        "        imgsz=640,\n",
        "        batch=16,\n",
        "        patience=15,\n",
        "        device=0,\n",
        "        project=os.path.join(WORKING_DIR, 'runs/detect'),\n",
        "        name='fire_smoke_detector',\n",
        "        exist_ok=True,\n",
        "        optimizer='SGD',\n",
        "        lr0=0.01,\n",
        "        lrf=0.01,\n",
        "        momentum=0.937,\n",
        "        weight_decay=0.0005,\n",
        "        warmup_epochs=3,\n",
        "        box=7.5,\n",
        "        cls=0.5,\n",
        "        dfl=1.5,\n",
        "        save=True,\n",
        "        val=True,\n",
        "        plots=True,\n",
        "        workers=4,\n",
        "        amp=True\n",
        "    )\n",
        "    print('Training complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step9_title",
      "metadata": {},
      "source": [
        "## Step 9: Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step9",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_path = os.path.join(WORKING_DIR, 'runs/detect/fire_smoke_detector/weights/best.pt')\n",
        "\n",
        "if os.path.exists(best_path):\n",
        "    eval_model = YOLO(best_path)\n",
        "    val_results = eval_model.val(\n",
        "        data=yaml_path,\n",
        "        imgsz=640,\n",
        "        batch=16,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    print('Performance Metrics:')\n",
        "    if hasattr(val_results, 'box'):\n",
        "        print(f\"mAP50: {val_results.box.map50:.4f}\")\n",
        "        print(f\"mAP50-95: {val_results.box.map:.4f}\")\n",
        "        print(f\"Precision: {val_results.box.mp:.4f}\")\n",
        "        print(f\"Recall: {val_results.box.mr:.4f}\")\n",
        "        \n",
        "        if hasattr(val_results.box, 'ap_class_index'):\n",
        "            print('Per-class mAP50:')\n",
        "            for i, cls_id in enumerate(val_results.box.ap_class_index):\n",
        "                name = CLASS_NAMES.get(int(cls_id), f\"Class {cls_id}\")\n",
        "                print(f\"  {name}: {val_results.box.ap50[i]:.4f}\")\n",
        "else:\n",
        "    print('Best model not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step10_title",
      "metadata": {},
      "source": [
        "## Step 10: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step10",
      "metadata": {},
      "outputs": [],
      "source": [
        "source_path = os.path.join(WORKING_DIR, 'runs/detect/fire_smoke_detector/weights/best.pt')\n",
        "final_path = os.path.join(MODELS_DIR, 'best.pt')\n",
        "\n",
        "if os.path.exists(source_path):\n",
        "    shutil.copy2(source_path, final_path)\n",
        "    size_mb = os.path.getsize(final_path) / (1024 * 1024)\n",
        "    \n",
        "    print(f\"Model saved: {final_path}\")\n",
        "    print(f\"Size: {size_mb:.2f} MB\")\n",
        "    \n",
        "    metadata = {\n",
        "        'model_name': f'FireSmoke_YOLOv8{MODEL_SIZE}',\n",
        "        'classes': CLASS_NAMES,\n",
        "        'num_classes': len(CLASS_NAMES),\n",
        "        'trained': datetime.now().isoformat(),\n",
        "        'framework': 'YOLOv8',\n",
        "        'image_size': 640,\n",
        "        'path': final_path,\n",
        "        'size_mb': round(size_mb, 2)\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(MODELS_DIR, 'metadata.json'), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    readme = f'''# Fire and Smoke Detection Model\n",
        "\n",
        "Model: YOLOv8{MODEL_SIZE}\n",
        "Classes: fire, smoke\n",
        "Size: {size_mb:.2f} MB\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from ultralytics import YOLO\n",
        "model = YOLO('best.pt')\n",
        "results = model.predict('image.jpg', conf=0.5)\n",
        "results[0].show()\n",
        "```\n",
        "'''\n",
        "    \n",
        "    with open(os.path.join(MODELS_DIR, 'README.md'), 'w') as f:\n",
        "        f.write(readme)\n",
        "    \n",
        "    print(f\"Metadata: {os.path.join(MODELS_DIR, 'metadata.json')}\")\n",
        "else:\n",
        "    print('Trained model not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step11_title",
      "metadata": {},
      "source": [
        "## Step 11: Test Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "step11",
      "metadata": {},
      "outputs": [],
      "source": [
        "if os.path.exists(final_path):\n",
        "    test_model = YOLO(final_path)\n",
        "    \n",
        "    test_dir = os.path.join(DATASET_PATH, 'images', 'test')\n",
        "    \n",
        "    if os.path.exists(test_dir):\n",
        "        samples = [f for f in os.listdir(test_dir) if f.endswith(('.jpg', '.png'))][:5]\n",
        "        \n",
        "        if samples:\n",
        "            print(f\"Testing on {len(samples)} images\\n\")\n",
        "            for img in samples:\n",
        "                path = os.path.join(test_dir, img)\n",
        "                results = test_model.predict(path, conf=0.5, save=True, \n",
        "                                            project=RESULTS_DIR, name='test')\n",
        "                \n",
        "                if results and len(results[0].boxes) > 0:\n",
        "                    print(f\"{img}: {len(results[0].boxes)} detections\")\n",
        "                    for box in results[0].boxes:\n",
        "                        cls = int(box.cls.item())\n",
        "                        conf = box.conf.item()\n",
        "                        name = CLASS_NAMES.get(cls, f'Class{cls}')\n",
        "                        print(f\"  {name}: {conf:.2%}\")\n",
        "                else:\n",
        "                    print(f\"{img}: no detections\")\n",
        "            print(f\"\\nResults saved: {RESULTS_DIR}/test/\")\n",
        "else:\n",
        "    print('Model not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Fire and smoke detection model trained and ready.\n",
        "\n",
        "Download model from:\n",
        "- Kaggle: /kaggle/working/models/fire_smoke/best.pt\n",
        "- Colab: /content/drive/MyDrive/safety-models/fire_smoke/best.pt\n",
        "- Local: ./models/fire_smoke/best.pt"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}